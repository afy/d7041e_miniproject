{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7041E Miniproject\n",
    "\n",
    "Group ID: MINI-PROJECT 14\n",
    "\n",
    "Ahmad Allahham\n",
    "[ahmall-0@student.ltu.se] | 940120-0556 |\n",
    "\n",
    "Arian Asghari\n",
    "[ariasg-0@student.ltu.se] | 010721-7051 |\n",
    "\n",
    "Hannes Furhoff\n",
    "[hanfur-0@student.ltu.se] | 010929-4710 |\n",
    "\n",
    "## Grade requirements\n",
    "### G3: Run and understand a publicly available model on a one selected dataset.\n",
    "We have chosen to work with the MLP model (regressive).\n",
    "\n",
    "### G3: Choose a dataset.\n",
    "Our data set is a collection of synthetic housing data, with various parameters (eg. rooms, year built) and price.\n",
    "\n",
    "### G3: Implement tutorial.\n",
    "NOT DONE YET\n",
    "\n",
    "### G3: Test performance for different configurations of the perceptron.\n",
    "Testing different configurations of the perceptron in terms of hidden layers, epochs, batch size, and learning rate. \n",
    "\n",
    "### G3: Document the performance.\n",
    "In README\n",
    "\n",
    "### G4: You should use data pre-processing\n",
    "For preprocessing a few steps were done:\n",
    "- Converting all values to float type.\n",
    "- Encoding the categorical \"neighborhood\" column to one-hot and then to float.\n",
    "\n",
    "### G4: Systematically choose the hyper-parameters of the model\n",
    "NOT DONE YET\n",
    "\n",
    "### G4: Use cross-validation for training \n",
    "NOT DONE YET\n",
    "\n",
    "### G4: Use different seeds and  recorded performance statistics with various performance metrics\n",
    "NOT DONE YET\n",
    "\n",
    "\n",
    "## Additional info\n",
    "Github link: `https://github.com/afy/d7041e_miniproject` <br/>\n",
    "Dataset link: `https://www.kaggle.com/datasets/muhammadbinimran/housing-price-prediction-data` <br/>\n",
    "Based on tutorial: `https://machinelearningmastery.com/building-a-regression-model-in-pytorch/` <br/>\n",
    "Youtube demo link: `NOT DONE YET` <br/>\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "import tqdm\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "\n",
    "# Returns from run\n",
    "class ModelMetrics:\n",
    "    def __init__(self, _mse, _rmse, _mae, _mape, _time, _settings):\n",
    "        self.mse = round(_mse, 5)\n",
    "        self.rmse =round( _rmse, 5)\n",
    "        self.mae = round(_mae, 5)\n",
    "        self.mape = round(_mape, 5)\n",
    "        self.training_time = round(_time, 2)\n",
    "        self.run_settings = _settings # The settings the model was run on\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"MSE={self.mse}, \"\n",
    "            f\"RMSE={self.rmse}, \"\n",
    "            f\"MAE={self.mae}, \"\n",
    "            f\"MAPE={self.mape}, \"\n",
    "            f\"time={self.training_time}s, \"\n",
    "            f\"settings={self.run_settings}\"\n",
    "        )\n",
    "\n",
    "# Settings given to the training function\n",
    "class ModelSettings:\n",
    "    def __init__(self, _hidden_layers, _epochs, _batch, _learning_rate, _k_folds):\n",
    "        self.hidden_layers = _hidden_layers\n",
    "        self.epochs = _epochs\n",
    "        self.batch_size = _batch\n",
    "        self.learning_rate = _learning_rate\n",
    "        self.k_folds = _k_folds\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"hidden_layers={self.hidden_layers}, \"\n",
    "            f\"epochs={self.epochs}, \"\n",
    "            f\"batch_size={self.batch_size}, \"\n",
    "            f\"learning_rate={self.learning_rate}, \"\n",
    "            f\"k_folds={self.k_folds}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "\n",
    "# Normalize\n",
    "def normalize(data):\n",
    "    normalized_data = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
    "    return normalized_data\n",
    "    \n",
    "# Read data\n",
    "raw_data = np.loadtxt(\"housing_data.csv\", delimiter=\",\", dtype=str)\n",
    "column_names = raw_data[0]\n",
    "raw_data = raw_data[1:]\n",
    "\n",
    "# For \"Neighborhood\" column:\n",
    "# Str -> OH-encoding -> str repr -> float repr \n",
    "# Eg. \"Urban\" -> [0 1 0] -> \"010\" -> (0)10.0\n",
    "d = raw_data[:,3].reshape(-1, 1)\n",
    "oh_enc = OneHotEncoder(dtype=int)\n",
    "oh_enc.fit(d)\n",
    "oh_d = oh_enc.transform(d).toarray().astype(str)\n",
    "for r in range(len(oh_d)):\n",
    "    tf = int(''.join(oh_d[r]))\n",
    "    raw_data[r,3] = tf\n",
    "\n",
    "# Type convert data\n",
    "raw_data = raw_data.astype(float)\n",
    "housing_data, housing_prices = normalize(raw_data[:,0:5]), normalize(raw_data[:,5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "class MLPRegressor:\n",
    "    def __init__(self, _input_size: int, _output_size: int, _settings: ModelSettings):\n",
    "        self.model = self.build_model(_input_size, _output_size, _settings.hidden_layers)\n",
    "        self.settings = _settings\n",
    "        \n",
    "    def build_model(self, input_size: int, output_size: int, hidden_layers) -> nn.Sequential:\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_layers[0])) \n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1])) \n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def convert_to_tensor(self, X_train, y_train, X_test, y_test):\n",
    "        X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "        y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "        X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "        y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "    def evaluate_model(self, X_train, y_train, X_test, y_test):\n",
    "        X_train, y_train, X_test, y_test = self.convert_to_tensor(X_train, y_train, X_test, y_test)\n",
    "        loss_fn = nn.MSELoss()  # mean square error\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr = self.settings.learning_rate)\n",
    "\n",
    "        n_epochs = self.settings.epochs   # Number of epochs to run\n",
    "        batch_size = self.settings.batch_size # Size of each batch\n",
    "        batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "        error = np.inf\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            self.model.train()\n",
    "            with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                bar.set_description(f\"Epoch {epoch}\")\n",
    "                for start in bar:\n",
    "                    # take a batch\n",
    "                    X_batch = X_train[start:start+batch_size]\n",
    "                    y_batch = y_train[start:start+batch_size]\n",
    "                    # forward pass\n",
    "                    y_pred = self.model(X_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "                    # backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # update weights\n",
    "                    optimizer.step()\n",
    "                    # print progress\n",
    "                    bar.set_postfix(mse=float(loss))\n",
    "            # evaluate accuracy at end of each epoch\n",
    "            self.model.eval()\n",
    "        with torch.no_grad():\n",
    "                y_pred = self.model(X_test)\n",
    "                mae = mean_absolute_error(y_pred, y_test)\n",
    "                mape = mean_absolute_percentage_error(y_pred, y_test)   \n",
    "                mse = loss_fn(y_pred, y_test)\n",
    "                rmse = torch.sqrt(mse)          \n",
    "        return mse, rmse, mae, mape\n",
    "\n",
    "    # K-Fold Cross-validation\n",
    "    def k_fold_validation(self, housing_data, housing_prices):\n",
    "        \n",
    "        stime = time.time()\n",
    "        kfold = KFold(n_splits=self.settings.k_folds, shuffle=True)\n",
    "        history = {\n",
    "            'mse': np.array([]),\n",
    "            'rmse': np.array([]),\n",
    "            'mae': np.array([]),\n",
    "            'mape': np.array([])\n",
    "        }\n",
    "        for fold, (train_ids, test_ids) in enumerate(kfold.split(housing_data)): \n",
    "            X_train = housing_data[train_ids]\n",
    "            X_test = housing_data[test_ids]\n",
    "            y_train = housing_prices[train_ids]\n",
    "            y_test = housing_prices[test_ids]\n",
    "            mse, rmse, mae, mape = self.evaluate_model(X_train, y_train, X_test, y_test)\n",
    "            history['mse'] = np.append(history['mse'], mse)\n",
    "            history['rmse'] = np.append(history['rmse'], rmse)\n",
    "            history['mae'] = np.append(history['mae'], mae)\n",
    "            history['mape'] = np.append(history['mape'], mape)\n",
    "        etime = time.time()\n",
    "        return ModelMetrics(\n",
    "            np.mean(history['mse']),\n",
    "            np.mean(history['rmse']),\n",
    "            np.mean(history['mae']),\n",
    "            np.mean(history['mape']),\n",
    "            etime - stime,\n",
    "            self.settings\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1: Score = 100.0%, Metrics=MSE=0, RMSE=0, MAE=0, MAPE=0, time=2s, settings=hidden_layers=[10, 10], epochs=3, batch_size=500, learning_rate=0.01, k_folds=5\n",
      "Rank: 2: Score = 100.0%, Metrics=MSE=0, RMSE=0, MAE=0, MAPE=0, time=2s, settings=hidden_layers=[10, 10], epochs=3, batch_size=500, learning_rate=0.05, k_folds=5\n",
      "Rank: 3: Score = 100.0%, Metrics=MSE=0, RMSE=0, MAE=0, MAPE=0, time=1s, settings=hidden_layers=[10, 10], epochs=3, batch_size=1000, learning_rate=0.01, k_folds=5\n",
      "Rank: 4: Score = 100.0%, Metrics=MSE=0, RMSE=0, MAE=0, MAPE=0, time=1s, settings=hidden_layers=[10, 10], epochs=3, batch_size=1000, learning_rate=0.05, k_folds=5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m\n\u001b[0;32m     25\u001b[0m settings \u001b[38;5;241m=\u001b[39m ModelSettings(\n\u001b[0;32m     26\u001b[0m     hidden_layers,\n\u001b[0;32m     27\u001b[0m     argcomb[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     argcomb[\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPRegressor(housing_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], housing_prices\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], settings)\n\u001b[1;32m---> 33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_fold_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhousing_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhousing_prices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m best_runs\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[0;32m     35\u001b[0m best_runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(best_runs, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mmape)[:max_saved_runs]\n",
      "Cell \u001b[1;32mIn[11], line 80\u001b[0m, in \u001b[0;36mMLPRegressor.k_fold_validation\u001b[1;34m(self, housing_data, housing_prices)\u001b[0m\n\u001b[0;32m     78\u001b[0m y_train \u001b[38;5;241m=\u001b[39m housing_prices[train_ids]\n\u001b[0;32m     79\u001b[0m y_test \u001b[38;5;241m=\u001b[39m housing_prices[test_ids]\n\u001b[1;32m---> 80\u001b[0m mse, rmse, mae, mape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m], mse)\n\u001b[0;32m     82\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m], rmse)\n",
      "Cell \u001b[1;32mIn[11], line 51\u001b[0m, in \u001b[0;36mMLPRegressor.evaluate_model\u001b[1;34m(self, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# print progress\u001b[39;00m\n\u001b[0;32m     53\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_postfix(mse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(loss))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:416\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    414\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[0;32m    419\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_get_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run\n",
    "\n",
    "# Running params\n",
    "# The idea is to cycle through the permutations of r\n",
    "k_folds = [5]                             # For cross-fold validation\n",
    "number_of_hidden_layers = [2, 3]                   # 1 'layer' = 1 neuron + activation in sequence\n",
    "learning_rates = [0.01, 0.05]\n",
    "batch_sizes = [500, 1000]\n",
    "epochs = [3, 10, 20, 60] # Epochs for training the model\n",
    "neurons_per_layer = [10, 50, 100, 200] # Neuros in each hidden layer\n",
    "\n",
    "# Other params\n",
    "max_saved_runs = 10\n",
    "\n",
    "# Setup\n",
    "best_runs = []\n",
    "\n",
    "# Train and test model for all argument combinations\n",
    "# Save the {max_saved_runs} best runs\n",
    "combos = itertools.product(number_of_hidden_layers, neurons_per_layer, epochs, batch_sizes, learning_rates, k_folds)\n",
    "\n",
    "\n",
    "for argcomb in combos:\n",
    "    hidden_layers = [argcomb[1] for i in range(argcomb[0])]\n",
    "    settings = ModelSettings(\n",
    "        hidden_layers,\n",
    "        argcomb[2],\n",
    "        argcomb[3],\n",
    "        argcomb[4],\n",
    "        argcomb[5]\n",
    "    )\n",
    "    mlp = MLPRegressor(housing_data.shape[1], housing_prices.shape[1], settings)\n",
    "    metrics = mlp.k_fold_validation(housing_data, housing_prices)\n",
    "    best_runs.append(metrics)\n",
    "    best_runs = sorted(best_runs, key=lambda x: x.mape)[:max_saved_runs]\n",
    "    clear_output(wait = True)\n",
    "    for i, metrics in enumerate(best_runs):\n",
    "        print(f\"Rank: {i + 1}: Score = {(1-metrics.mape)*100:.1f}%, Metrics={metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
