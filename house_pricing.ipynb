{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7041E Miniproject\n",
    "\n",
    "Group ID: MINI-PROJECT 14\n",
    "\n",
    "Ahmad Allahham\n",
    "[ahmall-0@student.ltu.se] | 940120-0556 |\n",
    "\n",
    "Arian Asghari\n",
    "[ariasg-0@student.ltu.se] | 010721-7051 |\n",
    "\n",
    "Hannes Furhoff\n",
    "[hanfur-0@student.ltu.se] | 010929-4710 |\n",
    "\n",
    "## Grade requirements\n",
    "### G3: Run and understand a publicly available model on a one selected dataset.\n",
    "We have chosen to work with the MLP model (regressive) on a housing dataset.\n",
    "\n",
    "### G3: Choose a dataset.\n",
    "Our data set is a collection of synthetic housing data, with various parameters (eg. rooms, year built) and price.\n",
    "\n",
    "### G3: Implement tutorial.\n",
    "Implemented a tutorial from machinelearningmastery, link is in README.md.\n",
    "\n",
    "### G3: Test performance for different configurations of the perceptron.\n",
    "Testing different configurations of the perceptron in terms of hidden layers, epochs, batch size, and learning rate. \n",
    "\n",
    "### G3: Document the performance.\n",
    "The performance metrics, including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE), are thoroughly documented. Please refer to the [Performance Table](#performance-table) for detailed results. \n",
    "\n",
    "### G4: You should use data pre-processing\n",
    "For preprocessing a few steps were done:\n",
    "- Converting all values to float type.\n",
    "- Encoding the categorical \"neighborhood\" column to one-hot and then to float.\n",
    "- Normalize float tables.\n",
    "\n",
    "### G4: Systematically choose the hyper-parameters of the model\n",
    "The hyperparameters of the MLP model, including the number of hidden layers, epochs, batch size, and learning rate, have been systematically chosen and tested. The impact of different configurations on model performance is detailed in the [Performance Table](#performance-table). \n",
    "\n",
    "### G4: Use cross-validation for training \n",
    "To ensure robust training and evaluate the model's generalization performance, we have employed k-fold cross-validation. The results of cross-validation are reflected in the [Performance Table](#performance-table) \n",
    "\n",
    "### G4: Use different seeds and  recorded performance statistics with various performance metrics\n",
    "We have utilized different random seeds during training and recorded comprehensive performance statistics, including MSE, RMSE, MAE, and MAPE. The [Performance Table](#performance-table) provides a detailed breakdown of these metrics for various model configurations. \n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "import tqdm\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "\n",
    "# Returns from run\n",
    "class ModelMetrics:\n",
    "    def __init__(self, _mse, _rmse, _mae, _mape, _time, _settings):\n",
    "        self.mse = round(_mse, 5)\n",
    "        self.rmse =round( _rmse, 5)\n",
    "        self.mae = round(_mae, 5)\n",
    "        self.mape = round(_mape, 5)\n",
    "        self.training_time = round(_time, 2)\n",
    "        self.run_settings = _settings # The settings the model was run on\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"MSE={self.mse}, \"\n",
    "            f\"RMSE={self.rmse}, \"\n",
    "            f\"MAE={self.mae}, \"\n",
    "            f\"MAPE={self.mape}, \"\n",
    "            f\"time={self.training_time}s, \"\n",
    "            f\"settings={self.run_settings}\"\n",
    "        )\n",
    "\n",
    "# Settings given to the training function\n",
    "class ModelSettings:\n",
    "    def __init__(self, _hidden_layers, _epochs, _batch, _learning_rate, _k_folds):\n",
    "        self.hidden_layers = _hidden_layers\n",
    "        self.epochs = _epochs\n",
    "        self.batch_size = _batch\n",
    "        self.learning_rate = _learning_rate\n",
    "        self.k_folds = _k_folds\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"hidden_layers={self.hidden_layers}, \"\n",
    "            f\"epochs={self.epochs}, \"\n",
    "            f\"batch_size={self.batch_size}, \"\n",
    "            f\"learning_rate={self.learning_rate}, \"\n",
    "            f\"k_folds={self.k_folds}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "\n",
    "# Normalize\n",
    "def normalize(data):\n",
    "    normalized_data = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
    "    return normalized_data\n",
    "    \n",
    "# Read data\n",
    "raw_data = np.loadtxt(\"housing_data.csv\", delimiter=\",\", dtype=str)\n",
    "column_names = raw_data[0]\n",
    "raw_data = raw_data[1:]\n",
    "\n",
    "# For \"Neighborhood\" column:\n",
    "# Str -> OH-encoding -> str repr -> float repr \n",
    "# Eg. \"Urban\" -> [0 1 0] -> \"010\" -> (0)10.0\n",
    "d = raw_data[:,3].reshape(-1, 1)\n",
    "oh_enc = OneHotEncoder(dtype=int)\n",
    "oh_enc.fit(d)\n",
    "oh_d = oh_enc.transform(d).toarray().astype(str)\n",
    "for r in range(len(oh_d)):\n",
    "    tf = int(''.join(oh_d[r]))\n",
    "    raw_data[r,3] = tf\n",
    "\n",
    "# Type convert data\n",
    "raw_data = raw_data.astype(float)\n",
    "housing_data, housing_prices = normalize(raw_data[:,0:5]), normalize(raw_data[:,5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "class MLPRegressor:\n",
    "    def __init__(self, _input_size: int, _output_size: int, _settings: ModelSettings):\n",
    "        self.model = self.build_model(_input_size, _output_size, _settings.hidden_layers)\n",
    "        self.settings = _settings\n",
    "        \n",
    "    def build_model(self, input_size: int, output_size: int, hidden_layers) -> nn.Sequential:\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_layers[0])) \n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1])) \n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def convert_to_tensor(self, X_train, y_train, X_test, y_test):\n",
    "        X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "        y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "        X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "        y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "    def evaluate_model(self, X_train, y_train, X_test, y_test):\n",
    "        X_train, y_train, X_test, y_test = self.convert_to_tensor(X_train, y_train, X_test, y_test)\n",
    "        loss_fn = nn.MSELoss()  # Mean square error\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr = self.settings.learning_rate)\n",
    "\n",
    "        n_epochs = self.settings.epochs   # Number of epochs to run\n",
    "        batch_size = self.settings.batch_size # Size of each batch\n",
    "        batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "        error = np.inf\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            self.model.train()\n",
    "            with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "                bar.set_description(f\"Epoch {epoch}\")\n",
    "                for start in bar:\n",
    "                    # Take a batch\n",
    "                    X_batch = X_train[start:start+batch_size]\n",
    "                    y_batch = y_train[start:start+batch_size]\n",
    "\n",
    "                    # Forward pass\n",
    "                    y_pred = self.model(X_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Update weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Print progress\n",
    "                    bar.set_postfix(mse=float(loss))\n",
    "\n",
    "            # Evaluate accuracy at end of each epoch\n",
    "            self.model.eval()\n",
    "        with torch.no_grad():\n",
    "                y_pred = self.model(X_test)\n",
    "                mae = mean_absolute_error(y_pred, y_test)\n",
    "                mape = mean_absolute_percentage_error(y_pred, y_test)   \n",
    "                mse = loss_fn(y_pred, y_test)\n",
    "                rmse = torch.sqrt(mse)          \n",
    "        return mse, rmse, mae, mape\n",
    "\n",
    "    # K-Fold Cross-validation\n",
    "    def k_fold_validation(self, housing_data, housing_prices):\n",
    "        \n",
    "        stime = time.time()\n",
    "        kfold = KFold(n_splits=self.settings.k_folds, shuffle=True)\n",
    "        history = {\n",
    "            'mse': np.array([]),\n",
    "            'rmse': np.array([]),\n",
    "            'mae': np.array([]),\n",
    "            'mape': np.array([])\n",
    "        }\n",
    "        for fold, (train_ids, test_ids) in enumerate(kfold.split(housing_data)): \n",
    "            X_train = housing_data[train_ids]\n",
    "            X_test = housing_data[test_ids]\n",
    "            y_train = housing_prices[train_ids]\n",
    "            y_test = housing_prices[test_ids]\n",
    "            mse, rmse, mae, mape = self.evaluate_model(X_train, y_train, X_test, y_test)\n",
    "            history['mse'] = np.append(history['mse'], mse)\n",
    "            history['rmse'] = np.append(history['rmse'], rmse)\n",
    "            history['mae'] = np.append(history['mae'], mae)\n",
    "            history['mape'] = np.append(history['mape'], mape)\n",
    "        etime = time.time()\n",
    "        return ModelMetrics(\n",
    "            np.mean(history['mse']),\n",
    "            np.mean(history['rmse']),\n",
    "            np.mean(history['mae']),\n",
    "            np.mean(history['mape']),\n",
    "            etime - stime,\n",
    "            self.settings\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1: Score = 84.2%, Metrics=MSE=0.00909, RMSE=0.09532, MAE=0.0762, MAPE=0.1585, time=17.71s, settings=hidden_layers=[10, 10, 10], epochs=60, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 2: Score = 84.1%, Metrics=MSE=0.00906, RMSE=0.0952, MAE=0.07611, MAPE=0.15853, time=12.68s, settings=hidden_layers=[200, 200], epochs=20, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 3: Score = 84.1%, Metrics=MSE=0.00905, RMSE=0.09511, MAE=0.07603, MAPE=0.15859, time=36.57s, settings=hidden_layers=[100, 100, 100], epochs=60, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 4: Score = 84.1%, Metrics=MSE=0.00913, RMSE=0.09555, MAE=0.07641, MAPE=0.15861, time=7.08s, settings=hidden_layers=[50, 50], epochs=20, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 5: Score = 84.1%, Metrics=MSE=0.00906, RMSE=0.0952, MAE=0.07609, MAPE=0.15875, time=14.75s, settings=hidden_layers=[10, 10], epochs=60, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 6: Score = 84.1%, Metrics=MSE=0.00904, RMSE=0.09508, MAE=0.07602, MAPE=0.15877, time=8.53s, settings=hidden_layers=[100, 100], epochs=20, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 7: Score = 84.1%, Metrics=MSE=0.00899, RMSE=0.09483, MAE=0.07581, MAPE=0.15889, time=4.85s, settings=hidden_layers=[10, 10], epochs=20, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 8: Score = 84.1%, Metrics=MSE=0.00905, RMSE=0.09512, MAE=0.07604, MAPE=0.15889, time=26.67s, settings=hidden_layers=[100, 100], epochs=60, batch_size=1000, learning_rate=0.05, k_folds=5\n",
      "Rank: 9: Score = 84.1%, Metrics=MSE=0.00899, RMSE=0.09482, MAE=0.0758, MAPE=0.15912, time=5.41s, settings=hidden_layers=[10, 10, 10], epochs=10, batch_size=500, learning_rate=0.01, k_folds=5\n",
      "Rank: 10: Score = 84.1%, Metrics=MSE=0.00901, RMSE=0.09491, MAE=0.07587, MAPE=0.15913, time=11.77s, settings=hidden_layers=[100, 100, 100], epochs=20, batch_size=1000, learning_rate=0.01, k_folds=5\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "\n",
    "# Running params\n",
    "# The idea is to cycle through the permutations of r\n",
    "k_folds = [5]                             # For cross-fold validation\n",
    "number_of_hidden_layers = [2, 3]          # 1 'layer' = 1 neuron + activation in sequence\n",
    "learning_rates = [0.01, 0.05]             # Learning rate of net optimizer\n",
    "batch_sizes = [500, 1000]                 # Batch size for training net with loss function\n",
    "epochs = [3, 10, 20, 60]                  # Epochs for training the model\n",
    "neurons_per_layer = [10, 50, 100, 200]    # Neuros in each hidden layer\n",
    "\n",
    "# Setup\n",
    "max_saved_runs = 10\n",
    "best_runs = []\n",
    "\n",
    "# Train and test model for all argument combinations\n",
    "# Save the {max_saved_runs} best runs\n",
    "combos = itertools.product(number_of_hidden_layers, neurons_per_layer, epochs, batch_sizes, learning_rates, k_folds)\n",
    "\n",
    "\n",
    "for argcomb in combos:\n",
    "    hidden_layers = [argcomb[1] for i in range(argcomb[0])]\n",
    "    settings = ModelSettings(\n",
    "        hidden_layers,\n",
    "        argcomb[2],\n",
    "        argcomb[3],\n",
    "        argcomb[4],\n",
    "        argcomb[5]\n",
    "    )\n",
    "    mlp = MLPRegressor(housing_data.shape[1], housing_prices.shape[1], settings)\n",
    "    metrics = mlp.k_fold_validation(housing_data, housing_prices)\n",
    "    best_runs.append(metrics)\n",
    "    best_runs = sorted(best_runs, key=lambda x: x.mape)[:max_saved_runs]\n",
    "\n",
    "    # Update output with new best 10\n",
    "    clear_output(wait = True)\n",
    "    for i, metrics in enumerate(best_runs):\n",
    "        print(f\"Rank: {i + 1}: Score = {(1-metrics.mape)*100:.1f}%, Metrics={metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
